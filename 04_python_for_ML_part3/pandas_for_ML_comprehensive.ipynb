{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pandas for Machine Learning - Comprehensive Guide\n",
        "## Intensive Session (Pure Pandas)\n",
        "\n",
        "### Course Overview\n",
        "This notebook covers pandas fundamentals through advanced topics, specifically designed for machine learning workflows using **pure pandas and numpy only**.\n",
        "\n",
        "**Topics Covered:**\n",
        "- Part 1: Pandas Fundamentals\n",
        "- Part 2: Data Manipulation & Analysis\n",
        "- Part 3: ML Data Preprocessing\n",
        "- Part 4: Advanced Techniques & Real ML Projects\n",
        "\n",
        "**Prerequisites:** Basic Python knowledge\n",
        "\n",
        "**Learning Outcomes:**\n",
        "- Master pandas for ML data preprocessing\n",
        "- Handle real-world datasets confidently\n",
        "- Build complete data preparation pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# PART 1: PANDAS FUNDAMENTALS\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 What is Pandas? (Theory)\n",
        "\n",
        "### üìö Introduction\n",
        "\n",
        "**Pandas** (Python Data Analysis Library) is the most popular library for data manipulation and analysis in Python.\n",
        "\n",
        "### üéØ Why Use Pandas?\n",
        "\n",
        "1. **Built for Data Science**\n",
        "   - Specifically designed for working with structured data (tables, spreadsheets, SQL tables)\n",
        "   - Industry standard for data preprocessing in ML pipelines\n",
        "\n",
        "2. **Easy to Use**\n",
        "   - Intuitive syntax similar to SQL and Excel\n",
        "   - Works seamlessly with NumPy, making ML integration smooth\n",
        "\n",
        "3. **Powerful Operations**\n",
        "   - Handle missing data effortlessly\n",
        "   - Group, aggregate, and transform data easily\n",
        "   - Merge and join datasets like SQL\n",
        "\n",
        "4. **Performance**\n",
        "   - Built on NumPy (C-optimized)\n",
        "   - Vectorized operations are extremely fast\n",
        "   - Can handle datasets with millions of rows\n",
        "\n",
        "### üîÑ Pandas vs Other Tools\n",
        "\n",
        "| Feature | Python Lists/Dicts | NumPy Arrays | Pandas |\n",
        "|---------|-------------------|--------------|--------|\n",
        "| Labeled Data | ‚ùå | ‚ùå | ‚úÖ |\n",
        "| Mixed Types | ‚úÖ | ‚ùå | ‚úÖ |\n",
        "| Missing Data | Manual | Manual | Built-in |\n",
        "| Data Alignment | Manual | Manual | Automatic |\n",
        "| SQL-like Operations | ‚ùå | ‚ùå | ‚úÖ |\n",
        "| Speed | Slow | Fast | Fast |\n",
        "\n",
        "### üéì When to Use Pandas?\n",
        "\n",
        "‚úÖ **Use Pandas when you need to:**\n",
        "- Load data from CSV, Excel, SQL, JSON\n",
        "- Clean and preprocess data for ML\n",
        "- Perform exploratory data analysis (EDA)\n",
        "- Handle missing values\n",
        "- Group and aggregate data\n",
        "- Merge multiple datasets\n",
        "\n",
        "‚ùå **Don't use Pandas for:**\n",
        "- Pure numerical computations (use NumPy)\n",
        "- Deep learning tensors (use PyTorch/TensorFlow)\n",
        "- Very large datasets that don't fit in memory (use Dask/PySpark)\n",
        "- Real-time streaming data (use specialized streaming tools)\n",
        "\n",
        "### üí° Key Concept\n",
        "\n",
        "> **Pandas is the bridge between raw data and machine learning models.**  \n",
        "> It transforms messy real-world data into clean, structured format that ML algorithms can understand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Setup and Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2.1 Creating DataFrames from Tuples\n",
        "\n",
        "Tuples are a common way to represent data rows. Here are various ways to create DataFrames using tuples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method 1: List of Tuples (each tuple is a row)\n",
        "print(\"Method 1: List of Tuples (rows)\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Student data as tuples\n",
        "students = [\n",
        "    ('Alice', 25, 85, 'CS'),\n",
        "    ('Bob', 22, 90, 'Math'),\n",
        "    ('Charlie', 24, 78, 'Physics'),\n",
        "    ('Diana', 23, 92, 'CS'),\n",
        "    ('Eve', 26, 88, 'Math')\n",
        "]\n",
        "\n",
        "# Create DataFrame with column names\n",
        "df_students = pd.DataFrame(students, columns=['Name', 'Age', 'Score', 'Major'])\n",
        "# pd.DataFrame: Creates a 2D labeled data structure (like a table/spreadsheet)\n",
        "print(df_students)\n",
        "print(f\"\\nShape: {df_students.shape}\")\n",
        "# .shape: Returns tuple (number_of_rows, number_of_columns)\n",
        "print(f\"Data types:\\n{df_students.dtypes}\")\n",
        "# .dtypes: Shows the data type of each column (int64, float64, object, etc.)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Method 2: Using from_records (alternative for tuples)\n",
        "print(\"Method 2: Using from_records()\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "employees = [\n",
        "    ('John', 30, 75000, 'Engineering'),\n",
        "    ('Sarah', 28, 82000, 'Data Science'),\n",
        "    ('Mike', 35, 68000, 'HR'),\n",
        "    ('Lisa', 32, 90000, 'Engineering')\n",
        "]\n",
        "\n",
        "df_employees = pd.DataFrame.from_records(\n",
        "# pd.DataFrame: Creates a 2D labeled data structure (like a table/spreadsheet)\n",
        "    employees, \n",
        "    columns=['Name', 'Age', 'Salary', 'Department']\n",
        ")\n",
        "print(df_employees)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Method 3: Dictionary of Tuples (columns as tuples)\n",
        "print(\"Method 3: Dictionary with Tuple Values\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Less common but valid - each key has a tuple of values\n",
        "products = {\n",
        "    'Product': ('Laptop', 'Phone', 'Tablet'),\n",
        "    'Price': (1200, 800, 500),\n",
        "    'Stock': (15, 50, 30)\n",
        "}\n",
        "\n",
        "df_products_tuple = pd.DataFrame(products)\n",
        "# pd.DataFrame: Creates a 2D labeled data structure (like a table/spreadsheet)\n",
        "print(df_products_tuple)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Method 4: Named Tuples (more structured approach)\n",
        "print(\"Method 4: Using Named Tuples\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "# Define structure\n",
        "Student = namedtuple('Student', ['name', 'age', 'grade', 'subject'])\n",
        "\n",
        "# Create named tuples\n",
        "students_named = [\n",
        "    Student('Alex', 20, 'A', 'Biology'),\n",
        "    Student('Beth', 21, 'B+', 'Chemistry'),\n",
        "    Student('Carl', 19, 'A-', 'Physics')\n",
        "]\n",
        "\n",
        "df_named = pd.DataFrame(students_named)\n",
        "# pd.DataFrame: Creates a 2D labeled data structure (like a table/spreadsheet)\n",
        "print(df_named)\n",
        "print(\"\\n‚úÖ Named tuples automatically provide column names!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Method 5: Single Tuple (one row)\n",
        "print(\"Method 5: Single Tuple (One Row)\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create DataFrame from single tuple\n",
        "single_record = ('Project Alpha', 150000, '2024-01-15', 'Completed')\n",
        "df_single = pd.DataFrame(\n",
        "# pd.DataFrame: Creates a 2D labeled data structure (like a table/spreadsheet)\n",
        "    [single_record],  # Wrap in list\n",
        "    columns=['Project', 'Budget', 'Date', 'Status']\n",
        ")\n",
        "print(df_single)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Real ML Example: Feature vectors as tuples\n",
        "print(\"ML Example: Feature Vectors as Tuples\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Each tuple represents features for one sample\n",
        "ml_samples = [\n",
        "    (5.1, 3.5, 1.4, 0.2, 'setosa'),\n",
        "    (7.0, 3.2, 4.7, 1.4, 'versicolor'),\n",
        "    (6.3, 3.3, 6.0, 2.5, 'virginica'),\n",
        "    (4.9, 3.0, 1.4, 0.2, 'setosa'),\n",
        "    (6.5, 2.8, 4.6, 1.5, 'versicolor')\n",
        "]\n",
        "\n",
        "df_iris = pd.DataFrame(\n",
        "# pd.DataFrame: Creates a 2D labeled data structure (like a table/spreadsheet)\n",
        "    ml_samples,\n",
        "    columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
        ")\n",
        "print(df_iris)\n",
        "print(f\"\\n‚úÖ Perfect for ML: {df_iris.shape[0]} samples, {df_iris.shape[1]-1} features\")\n",
        "# .shape: Returns tuple (number_of_rows, number_of_columns)\n",
        "\n",
        "# Separate features and target\n",
        "X = df_iris[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
        "y = df_iris['species']\n",
        "print(f\"\\nFeatures shape: {X.shape}\")\n",
        "# .shape: Returns tuple (number_of_rows, number_of_columns)\n",
        "print(f\"Target shape: {y.shape}\")\n",
        "# .shape: Returns tuple (number_of_rows, number_of_columns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Key Takeaways: DataFrames from Tuples\n",
        "\n",
        "**When to Use Each Method:**\n",
        "\n",
        "| Method | Best For | Example |\n",
        "|--------|----------|---------|\n",
        "| **List of Tuples** | Row-based data, CSV-like structure | `[(row1), (row2), ...]` |\n",
        "| **from_records()** | Explicit records/rows | `DataFrame.from_records(tuples)` |\n",
        "| **Dict of Tuples** | Column-based data | `{'col1': (v1, v2), 'col2': ...}` |\n",
        "| **Named Tuples** | Structured data with field names | `namedtuple('Student', fields)` |\n",
        "\n",
        "**Advantages of Tuples:**\n",
        "- ‚úÖ **Immutable**: Data can't be accidentally changed\n",
        "- ‚úÖ **Memory efficient**: Less memory than lists\n",
        "- ‚úÖ **Fast**: Better performance for read-only data\n",
        "- ‚úÖ **Clean syntax**: `(val1, val2, val3)` is concise\n",
        "- ‚úÖ **Type safety**: Named tuples add structure\n",
        "\n",
        "**Common Use Cases in ML:**\n",
        "```python\n",
        "# Feature vectors from database\n",
        "samples = [(feature1, feature2, feature3, label), ...]\n",
        "\n",
        "# Model predictions with metadata  \n",
        "results = [(sample_id, prediction, confidence), ...]\n",
        "\n",
        "# Hyperparameter combinations\n",
        "params = [(lr1, n_est1, depth1), (lr2, n_est2, depth2), ...]\n",
        "```\n",
        "\n",
        "**Pro Tip:** Use tuples when your data comes from:\n",
        "- Database queries (often return tuples)\n",
        "- CSV readers (rows as tuples)\n",
        "- External APIs (structured responses)\n",
        "- Fixed configuration data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2.2 Creating DataFrames with Custom Index\n",
        "\n",
        "By default, pandas creates a numeric index (0, 1, 2, ...). But you can create **custom indices** that are more meaningful for your data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method 1: Set index during DataFrame creation\n",
        "print(\"Method 1: Custom Index During Creation\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "data = {\n",
        "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana'],\n",
        "    'Score': [85, 92, 78, 95],\n",
        "    'Grade': ['B', 'A', 'C', 'A']\n",
        "}\n",
        "\n",
        "# Use custom student IDs as index\n",
        "df_students = pd.DataFrame(data, index=['STU001', 'STU002', 'STU003', 'STU004'])\n",
        "# pd.DataFrame: Creates a 2D labeled data structure (like a table/spreadsheet)\n",
        "print(df_students)\n",
        "print(f\"\\nIndex: {df_students.index.tolist()}\")\n",
        "# .index: Get the index (row labels)\n",
        "print(f\"Access by custom index: df.loc['STU002'] =\")\n",
        "# .loc: Label-based indexing (access by index/column names)\n",
        "print(df_students.loc['STU002'])\n",
        "# .loc: Label-based indexing (access by index/column names)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# Method 2: Using range with custom start/step\n",
        "print(\"Method 2: Custom Numeric Index (Range)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Start from 100, step by 10\n",
        "data = {\n",
        "    'Product': ['Laptop', 'Mouse', 'Keyboard'],\n",
        "    'Price': [1200, 25, 75]\n",
        "}\n",
        "\n",
        "df_products = pd.DataFrame(data, index=range(100, 130, 10))\n",
        "# pd.DataFrame: Creates a 2D labeled data structure (like a table/spreadsheet)\n",
        "print(df_products)\n",
        "print(f\"\\nIndex values: {df_products.index.tolist()}\")\n",
        "# .index: Get the index (row labels)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# Method 3: String-based meaningful indices\n",
        "print(\"Method 3: Descriptive String Index\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "cities_data = {\n",
        "    'Population': [8_336_817, 3_979_576, 2_693_976, 2_320_268],\n",
        "    'Area_km2': [783.8, 1213.8, 606.1, 369.2],\n",
        "    'Density': [10636, 3279, 4447, 6283]\n",
        "}\n",
        "\n",
        "df_cities = pd.DataFrame(\n",
        "# pd.DataFrame: Creates a 2D labeled data structure (like a table/spreadsheet)\n",
        "    cities_data, \n",
        "    index=['New York', 'Los Angeles', 'Chicago', 'Houston']\n",
        ")\n",
        "print(df_cities)\n",
        "print(f\"\\nAccess Chicago: \\n{df_cities.loc['Chicago']}\")\n",
        "# .loc: Label-based indexing (access by index/column names)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# Method 4: Date-based index (Time Series)\n",
        "print(\"Method 4: Date-Based Index (Time Series)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create date range as index\n",
        "dates = pd.date_range('2024-01-01', periods=7, freq='D')\n",
        "# pd.date_range(): Create sequence of dates with specified frequency\n",
        "\n",
        "sales_data = {\n",
        "    'Revenue': [1200, 1500, 1100, 1800, 2000, 1700, 1900],\n",
        "    'Orders': [45, 52, 38, 61, 73, 58, 67]\n",
        "}\n",
        "\n",
        "df_sales = pd.DataFrame(sales_data, index=dates)\n",
        "# pd.DataFrame: Creates a 2D labeled data structure (like a table/spreadsheet)\n",
        "print(df_sales)\n",
        "print(f\"\\nIndex type: {type(df_sales.index)}\")\n",
        "# .index: Get the index (row labels)\n",
        "print(f\"Access by date: df.loc['2024-01-03'] =\")\n",
        "# .loc: Label-based indexing (access by index/column names)\n",
        "print(df_sales.loc['2024-01-03'])\n",
        "# .loc: Label-based indexing (access by index/column names)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# Method 5: Set index from existing column\n",
        "print(\"Method 5: Set Index from Column (set_index)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create with default index first\n",
        "employees = pd.DataFrame({\n",
        "# pd.DataFrame: Creates a 2D labeled data structure (like a table/spreadsheet)\n",
        "    'EmployeeID': ['E001', 'E002', 'E003', 'E004'],\n",
        "    'Name': ['John', 'Sarah', 'Mike', 'Lisa'],\n",
        "    'Department': ['IT', 'HR', 'IT', 'Finance'],\n",
        "    'Salary': [75000, 65000, 80000, 70000]\n",
        "})\n",
        "\n",
        "print(\"Before setting index:\")\n",
        "print(employees)\n",
        "\n",
        "# Make EmployeeID the index\n",
        "df_employees = employees.set_index('EmployeeID')\n",
        "# .set_index(): Set a column as the new index\n",
        "print(\"\\nAfter setting EmployeeID as index:\")\n",
        "print(df_employees)\n",
        "print(f\"\\nAccess by EmployeeID: df.loc['E003'] =\")\n",
        "# .loc: Label-based indexing (access by index/column names)\n",
        "print(df_employees.loc['E003'])\n",
        "# .loc: Label-based indexing (access by index/column names)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# Method 6: Multi-level (Hierarchical) Index\n",
        "print(\"Method 6: Multi-Level Index (Advanced)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create multi-level index from tuples\n",
        "index = pd.MultiIndex.from_tuples([\n",
        "# pd.MultiIndex: Create hierarchical/multi-level index\n",
        "    ('Q1', 'Jan'), ('Q1', 'Feb'), ('Q1', 'Mar'),\n",
        "    ('Q2', 'Apr'), ('Q2', 'May'), ('Q2', 'Jun')\n",
        "], names=['Quarter', 'Month'])\n",
        "\n",
        "monthly_data = pd.DataFrame({\n",
        "# pd.DataFrame: Creates a 2D labeled data structure (like a table/spreadsheet)\n",
        "    'Sales': [100, 120, 115, 130, 140, 135],\n",
        "    'Profit': [20, 25, 23, 28, 32, 30]\n",
        "}, index=index)\n",
        "\n",
        "print(monthly_data)\n",
        "print(\"\\nAccess Q1 data:\")\n",
        "print(monthly_data.loc['Q1'])\n",
        "# .loc: Label-based indexing (access by index/column names)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# Method 7: ML Example - Sample IDs as Index\n",
        "print(\"Method 7: ML Example - Sample/Patient IDs\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Medical dataset with patient IDs\n",
        "ml_data = {\n",
        "    'Age': [45, 38, 52, 61, 33],\n",
        "    'Blood_Pressure': [120, 110, 140, 150, 105],\n",
        "    'Cholesterol': [200, 180, 240, 260, 170],\n",
        "    'Diagnosis': ['Healthy', 'Healthy', 'At Risk', 'At Risk', 'Healthy']\n",
        "}\n",
        "\n",
        "# Use patient IDs as index\n",
        "patient_ids = ['P1001', 'P1002', 'P1003', 'P1004', 'P1005']\n",
        "df_patients = pd.DataFrame(ml_data, index=patient_ids)\n",
        "# pd.DataFrame: Creates a 2D labeled data structure (like a table/spreadsheet)\n",
        "\n",
        "print(df_patients)\n",
        "print(f\"\\nFeatures for patient P1003:\")\n",
        "print(df_patients.loc['P1003'])\n",
        "# .loc: Label-based indexing (access by index/column names)\n",
        "\n",
        "# Separate features and target while keeping index\n",
        "X = df_patients[['Age', 'Blood_Pressure', 'Cholesterol']]\n",
        "y = df_patients['Diagnosis']\n",
        "\n",
        "print(f\"\\nX shape: {X.shape}, Index preserved: {X.index.tolist()}\")\n",
        "# .shape: Returns tuple (number_of_rows, number_of_columns)\n",
        "print(f\"y shape: {y.shape}, Index preserved: {y.index.tolist()}\")\n",
        "# .shape: Returns tuple (number_of_rows, number_of_columns)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# Method 8: Reset and Modify Index\n",
        "print(\"Method 8: Resetting and Modifying Index\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "df = pd.DataFrame({\n",
        "# pd.DataFrame: Creates a 2D labeled data structure (like a table/spreadsheet)\n",
        "    'City': ['NYC', 'LA', 'Chicago'],\n",
        "    'Population': [8.3, 3.9, 2.7]\n",
        "}, index=['A', 'B', 'C'])\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "print(df)\n",
        "\n",
        "# Reset to default numeric index\n",
        "df_reset = df.reset_index(drop=True)\n",
        "# .reset_index(): Reset index to default 0,1,2... numbering\n",
        "print(\"\\nAfter reset_index(drop=True):\")\n",
        "# drop=True: Don't keep the old index as a new column\n",
        "print(df_reset)\n",
        "\n",
        "# Reset but keep old index as column\n",
        "df_reset_keep = df.reset_index()\n",
        "# .reset_index(): Reset index to default 0,1,2... numbering\n",
        "print(\"\\nAfter reset_index() - keeps old index as column:\")\n",
        "print(df_reset_keep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Key Takeaways: Custom Indices\n",
        "\n",
        "**Why Use Custom Index?**\n",
        "\n",
        "| Benefit | Description | Example |\n",
        "|---------|-------------|---------|\n",
        "| **Meaningful Access** | Access rows by meaningful labels | `df.loc['NYC']` instead of `df.iloc[0]` |\n",
        "| **Data Integrity** | Keep track of samples/records | Patient IDs, Transaction IDs |\n",
        "| **Time Series** | Date-based operations | Resampling, rolling windows |\n",
        "| **Joins/Merges** | Align data by index | Merge datasets on common IDs |\n",
        "| **Readability** | Self-documenting code | Clear what each row represents |\n",
        "\n",
        "**Index Methods Summary:**\n",
        "\n",
        "```python\n",
        "# During creation\n",
        "df = pd.DataFrame(data, index=custom_index)\n",
        "\n",
        "# From existing column\n",
        "df = df.set_index('column_name')\n",
        "\n",
        "# Date range\n",
        "df = pd.DataFrame(data, index=pd.date_range('2024-01-01', periods=10))\n",
        "\n",
        "# Range with custom start/step\n",
        "df = pd.DataFrame(data, index=range(100, 200, 10))\n",
        "\n",
        "# Multi-level\n",
        "index = pd.MultiIndex.from_tuples([(level1, level2), ...])\n",
        "df = pd.DataFrame(data, index=index)\n",
        "\n",
        "# Reset to default\n",
        "df = df.reset_index(drop=True)\n",
        "```\n",
        "\n",
        "**When to Use Custom Index:**\n",
        "\n",
        "‚úÖ **Use Custom Index When:**\n",
        "- Working with time series data (dates as index)\n",
        "- Data has natural unique identifiers (IDs, codes)\n",
        "- Need to merge/join datasets frequently\n",
        "- Want more readable code\n",
        "- Tracking specific samples/entities\n",
        "\n",
        "‚ùå **Use Default Index When:**\n",
        "- Data is sequential/ordered\n",
        "- No natural identifier exists\n",
        "- Doing matrix operations (reset before converting to NumPy)\n",
        "- Index doesn't add value\n",
        "\n",
        "**ML Best Practices:**\n",
        "\n",
        "```python\n",
        "# ‚úÖ GOOD: Keep sample IDs as index during preprocessing\n",
        "df = pd.DataFrame(features, index=sample_ids)\n",
        "X_train, X_test = train_test_split(df, ...)  # Index preserved!\n",
        "\n",
        "# ‚úÖ GOOD: Track which samples during feature engineering\n",
        "df['new_feature'] = df['feature1'] / df['feature2']\n",
        "# Can still identify which sample each row belongs to\n",
        "\n",
        "# ‚ö†Ô∏è WARNING: Some sklearn functions require reset\n",
        "# If you get errors, try:\n",
        "X_train_reset = X_train.reset_index(drop=True)\n",
        "model.fit(X_train_reset, y_train)\n",
        "\n",
        "# üí° TIP: Save index before converting to NumPy\n",
        "index_backup = df.index\n",
        "X_array = df.values  # Now NumPy array\n",
        "# Can restore later: pd.DataFrame(X_array, index=index_backup)\n",
        "```\n",
        "\n",
        "**Common Index Operations:**\n",
        "\n",
        "```python\n",
        "# Check index\n",
        "df.index                    # View index\n",
        "df.index.name = 'ID'       # Name the index\n",
        "df.index.is_unique         # Check if unique\n",
        "\n",
        "# Modify index\n",
        "df.index = new_index       # Replace index\n",
        "df.rename(index={'old': 'new'})  # Rename specific values\n",
        "df.sort_index()            # Sort by index\n",
        "\n",
        "# Multi-index\n",
        "df.index.levels            # View levels\n",
        "df.xs('value', level=0)    # Cross-section selection\n",
        "df.unstack()               # Convert multi-index to columns\n",
        "```\n",
        "\n",
        "**Real-World Example:**\n",
        "\n",
        "```python\n",
        "# Customer purchase tracking\n",
        "purchases = pd.DataFrame({\n",
        "    'Amount': [50, 120, 75],\n",
        "    'Product': ['Book', 'Phone', 'Laptop']\n",
        "}, index=['TXN001', 'TXN002', 'TXN003'])\n",
        "\n",
        "# Easy lookup\n",
        "purchase_details = purchases.loc['TXN002']\n",
        "\n",
        "# Merging with customer data becomes natural\n",
        "customer_df.join(purchases, on='transaction_id')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pandas version: 1.5.3\n",
            "NumPy version: 1.24.3\n",
            "\n",
            "‚úÖ Environment ready for ML data processing!\n"
          ]
        }
      ],
      "source": [
        "# Essential imports - Only pandas and numpy!\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# warnings.filterwarnings(): Control which warnings are displayed\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "# pd.set_option(): Configure pandas display settings\n",
        "pd.set_option('display.max_rows', 100)\n",
        "# pd.set_option(): Configure pandas display settings\n",
        "pd.set_option('display.precision', 2)\n",
        "# pd.set_option(): Configure pandas display settings\n",
        "pd.set_option('display.width', 100)\n",
        "# pd.set_option(): Configure pandas display settings\n",
        "\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(\"\\n‚úÖ Environment ready for ML data processing!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 Understanding Pandas Series (Theory)\n",
        "\n",
        "### üìä What is a Series?\n",
        "\n",
        "A **Series** is a **one-dimensional labeled array** that can hold any data type (integers, strings, floats, objects, etc.).\n",
        "\n",
        "```\n",
        "Think of it as:\n",
        "- A single column from a spreadsheet\n",
        "- A Python list with labels/index\n",
        "- A dictionary that maintains order\n",
        "- A 1D NumPy array with labels\n",
        "```\n",
        "\n",
        "### üèóÔ∏è Structure of a Series\n",
        "\n",
        "```\n",
        "Index (Labels)    Values\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "0                 100\n",
        "1                 200\n",
        "2                 300\n",
        "3                 400\n",
        "```\n",
        "\n",
        "### üéØ Why Use Series?\n",
        "\n",
        "1. **Labeled Access**\n",
        "   - Access data by meaningful labels, not just positions\n",
        "   - Example: `prices['Laptop']` instead of `prices[0]`\n",
        "\n",
        "2. **Automatic Alignment**\n",
        "   - Operations align based on labels, not positions\n",
        "   - Missing labels automatically get NaN\n",
        "\n",
        "3. **Built-in Operations**\n",
        "   - Statistical functions: `.mean()`, `.std()`, `.max()`\n",
        "   - No need to write loops\n",
        "\n",
        "4. **Handles Missing Data**\n",
        "   - Built-in support for `NaN` (Not a Number)\n",
        "   - Functions automatically skip `NaN` values\n",
        "\n",
        "### üìù When to Use Series?\n",
        "\n",
        "‚úÖ **Use Series when you have:**\n",
        "- A single column of data\n",
        "- Time series data (stock prices over time)\n",
        "- A list with meaningful labels\n",
        "- Need to perform statistical operations\n",
        "\n",
        "‚ùå **Don't use Series when:**\n",
        "- You need multiple columns (use DataFrame)\n",
        "- You need 2D operations (use DataFrame or NumPy)\n",
        "\n",
        "### üí° Key Characteristics\n",
        "\n",
        "| Property | Description | Example |\n",
        "|----------|-------------|----------|\n",
        "| **Index** | Labels for each value | `['A', 'B', 'C']` or `[0, 1, 2]` |\n",
        "| **Values** | Actual data (NumPy array) | `[10, 20, 30]` |\n",
        "| **Name** | Optional name for the series | `'Prices'` |\n",
        "| **dtype** | Data type of values | `int64`, `float64`, `object` |\n",
        "\n",
        "### üîç Series vs List vs NumPy Array\n",
        "\n",
        "```python\n",
        "# Python List\n",
        "prices_list = [100, 200, 300]\n",
        "# Access: prices_list[0] = 100\n",
        "# No labels, manual operations\n",
        "\n",
        "# NumPy Array\n",
        "prices_array = np.array([100, 200, 300])\n",
        "# Access: prices_array[0] = 100\n",
        "# Fast, but no labels\n",
        "\n",
        "# Pandas Series\n",
        "prices_series = pd.Series([100, 200, 300], index=['A', 'B', 'C'])\n",
        "# Access: prices_series['A'] = 100\n",
        "# Labeled + Fast + Built-in operations\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.4 Series - Practical Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Series Example:\n",
            "Laptop      1200\n",
            "Mouse         25\n",
            "Keyboard      75\n",
            "Monitor      300\n",
            "Name: Price, dtype: int64\n",
            "\n",
            "Mean price: $400.00\n",
            "Most expensive: Laptop at $1200\n",
            "Type: <class 'pandas.core.series.Series'>\n"
          ]
        }
      ],
      "source": [
        "# Series: 1D labeled array (like a column)\n",
        "prices = pd.Series([1200, 25, 75, 300], \n",
        "# pd.Series: Creates a 1D labeled array (like a single column)\n",
        "                   index=['Laptop', 'Mouse', 'Keyboard', 'Monitor'],\n",
        "                   name='Price')\n",
        "print(\"Series Example:\")\n",
        "print(prices)\n",
        "print(f\"\\nMean price: ${prices.mean():.2f}\")\n",
        "# .mean(): Calculate the average value (sum divided by count)\n",
        "print(f\"Most expensive: {prices.idxmax()} at ${prices.max()}\")\n",
        "# .max(): Find the maximum (largest) value\n",
        "print(f\"Type: {type(prices)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.5 Understanding Pandas DataFrame (Theory)\n",
        "\n",
        "### üìä What is a DataFrame?\n",
        "\n",
        "A **DataFrame** is a **two-dimensional labeled data structure** with columns of potentially different types.\n",
        "\n",
        "```\n",
        "Think of it as:\n",
        "- An Excel spreadsheet\n",
        "- A SQL table\n",
        "- A dictionary of Series\n",
        "- A collection of columns, each being a Series\n",
        "```\n",
        "\n",
        "### üèóÔ∏è Structure of a DataFrame\n",
        "\n",
        "```\n",
        "        Column1  Column2  Column3\n",
        "Index\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "0         A        100      True\n",
        "1         B        200      False\n",
        "2         C        300      True\n",
        "```\n",
        "\n",
        "### üéØ Why Use DataFrame?\n",
        "\n",
        "1. **Multi-Column Data**\n",
        "   - Store different types of data together\n",
        "   - Each column can have different data type\n",
        "   - Real-world datasets are always multi-column\n",
        "\n",
        "2. **Two-Dimensional Operations**\n",
        "   - Filter rows based on conditions\n",
        "   - Select specific columns\n",
        "   - Aggregate across rows or columns\n",
        "\n",
        "3. **SQL-Like Operations**\n",
        "   - Group by columns\n",
        "   - Join/merge multiple datasets\n",
        "   - Pivot and reshape data\n",
        "\n",
        "4. **ML-Ready Format**\n",
        "   - Direct input to most ML libraries\n",
        "   - Easy feature engineering\n",
        "   - Train-test split compatible\n",
        "\n",
        "### üìù When to Use DataFrame?\n",
        "\n",
        "‚úÖ **Use DataFrame when you have:**\n",
        "- Multiple columns of data\n",
        "- Datasets from CSV, Excel, SQL, JSON\n",
        "- Need to perform data analysis\n",
        "- Preparing data for machine learning\n",
        "- Multiple features for each observation\n",
        "\n",
        "‚ùå **Don't use DataFrame when:**\n",
        "- Single column (use Series)\n",
        "- 3D+ data (use NumPy or xarray)\n",
        "- Pure numerical matrix operations (use NumPy)\n",
        "\n",
        "### üí° Key Components\n",
        "\n",
        "| Component | Description | Access Method |\n",
        "|-----------|-------------|---------------|\n",
        "| **Index** | Row labels | `df.index` |\n",
        "| **Columns** | Column names | `df.columns` |\n",
        "| **Values** | 2D NumPy array | `df.values` |\n",
        "| **dtypes** | Data types per column | `df.dtypes` |\n",
        "| **Shape** | (rows, columns) | `df.shape` |\n",
        "\n",
        "### üîç DataFrame Analogy\n",
        "\n",
        "```\n",
        "DataFrame = Dictionary of Series\n",
        "\n",
        "{\n",
        "    'Name': Series(['Alice', 'Bob', 'Charlie']),\n",
        "    'Age': Series([25, 30, 35]),\n",
        "    'Salary': Series([50000, 60000, 70000])\n",
        "}\n",
        "\n",
        "Each Series (column) has the same index (rows).\n",
        "```\n",
        "\n",
        "### üìä DataFrame vs Other Structures\n",
        "\n",
        "| Feature | List of Lists | NumPy 2D Array | DataFrame |\n",
        "|---------|---------------|----------------|------------|\n",
        "| Column Names | ‚ùå | ‚ùå | ‚úÖ |\n",
        "| Row Labels | ‚ùå | ‚ùå | ‚úÖ |\n",
        "| Mixed Types | ‚úÖ | ‚ùå | ‚úÖ |\n",
        "| Missing Data | Manual | Manual | Built-in |\n",
        "| Column Selection | Manual | Slicing | By Name |\n",
        "| SQL-like Ops | ‚ùå | ‚ùå | ‚úÖ |\n",
        "\n",
        "### üéì Real-World Use Case\n",
        "\n",
        "```\n",
        "Imagine you have customer data:\n",
        "\n",
        "CustomerID | Name    | Age | Purchase | City\n",
        "-----------|---------|-----|----------|----------\n",
        "1          | Alice   | 25  | 1200     | NYC\n",
        "2          | Bob     | 30  | 800      | LA\n",
        "3          | Charlie | 35  | 1500     | Chicago\n",
        "\n",
        "This is a DataFrame!\n",
        "- Rows: Individual customers\n",
        "- Columns: Different attributes (mixed types)\n",
        "- Index: Can be CustomerID or default 0,1,2\n",
        "```\n",
        "\n",
        "### üí™ Why DataFrames are Perfect for ML\n",
        "\n",
        "1. **Features as Columns**: Each ML feature is a column\n",
        "2. **Samples as Rows**: Each observation is a row\n",
        "3. **Easy Preprocessing**: Built-in functions for cleaning\n",
        "4. **Compatibility**: Works with sklearn, tensorflow, pytorch\n",
        "5. **Exploration**: Easy to inspect and understand data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.6 DataFrame - Practical Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame Example:\n",
            "    Product  Price  Stock     Category\n",
            "0    Laptop   1200     15  Electronics\n",
            "1     Mouse     25    100  Accessories\n",
            "2  Keyboard     75     50  Accessories\n",
            "3   Monitor    300     30  Electronics\n",
            "4    Webcam     80     45  Accessories\n",
            "\n",
            "Shape: (5, 4) (rows, columns)\n",
            "Columns: ['Product', 'Price', 'Stock', 'Category']\n",
            "Data types:\n",
            "Product     object\n",
            "Price        int64\n",
            "Stock        int64\n",
            "Category    object\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "# DataFrame: 2D table (multiple columns)\n",
        "products_data = {\n",
        "    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam'],\n",
        "    'Price': [1200, 25, 75, 300, 80],\n",
        "    'Stock': [15, 100, 50, 30, 45],\n",
        "    'Category': ['Electronics', 'Accessories', 'Accessories', 'Electronics', 'Accessories']\n",
        "}\n",
        "\n",
        "df_products = pd.DataFrame(products_data)\n",
        "# pd.DataFrame: Creates a 2D labeled data structure (like a table/spreadsheet)\n",
        "print(\"DataFrame Example:\")\n",
        "print(df_products)\n",
        "print(f\"\\nShape: {df_products.shape} (rows, columns)\")\n",
        "# .shape: Returns tuple (number_of_rows, number_of_columns)\n",
        "print(f\"Columns: {list(df_products.columns)}\")\n",
        "# .columns: Get list of column names\n",
        "print(f\"Data types:\\n{df_products.dtypes}\")\n",
        "# .dtypes: Shows the data type of each column (int64, float64, object, etc.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 Creating ML Dataset - Iris Flowers (Manual)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üå∏ Iris Dataset - Classic ML Dataset\n",
            "Shape: (150, 5)\n",
            "\n",
            "First 10 rows:\n",
            "   sepal_length  sepal_width  petal_length  petal_width     species\n",
            "0          5.06         2.80          5.28         1.49  versicolor\n",
            "1          4.68         3.54          1.69         0.22      setosa\n",
            "2          7.35         2.83          5.55         1.49   virginica\n",
            "3          5.53         2.82          4.59         1.74  versicolor\n",
            "4          5.93         2.86          4.84         1.47  versicolor\n",
            "5          5.65         3.54          1.51         0.16      setosa\n",
            "6          5.74         2.72          4.91         1.44  versicolor\n",
            "7          6.58         3.47          4.88         1.87   virginica\n",
            "8          5.65         2.66          4.45         1.32  versicolor\n",
            "9          5.92         3.29          4.34         0.88  versicolor\n",
            "\n",
            "Species distribution:\n",
            "versicolor    50\n",
            "setosa        50\n",
            "virginica     50\n",
            "Name: species, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Create Iris dataset manually (famous ML dataset)\n",
        "np.random.seed(42)\n",
        "# np.random: NumPy random number generation functions\n",
        "\n",
        "# Generate realistic iris data\n",
        "n_samples = 150\n",
        "n_per_class = 50\n",
        "\n",
        "# Setosa (smaller flowers)\n",
        "setosa = pd.DataFrame({\n",
        "# pd.DataFrame: Creates a 2D labeled data structure (like a table/spreadsheet)\n",
        "    'sepal_length': np.random.normal(5.0, 0.35, n_per_class),\n",
        "    # np.random: NumPy random number generation functions\n",
        "    'sepal_width': np.random.normal(3.4, 0.38, n_per_class),\n",
        "    # np.random: NumPy random number generation functions\n",
        "    'petal_length': np.random.normal(1.5, 0.17, n_per_class),\n",
        "    # np.random: NumPy random number generation functions\n",
        "    'petal_width': np.random.normal(0.25, 0.11, n_per_class),\n",
        "    # np.random: NumPy random number generation functions\n",
        "    'species': 'setosa'\n",
        "})\n",
        "\n",
        "# Versicolor (medium flowers)\n",
        "versicolor = pd.DataFrame({\n",
        "# pd.DataFrame: Creates a 2D labeled data structure (like a table/spreadsheet)\n",
        "    'sepal_length': np.random.normal(5.9, 0.52, n_per_class),\n",
        "    # np.random: NumPy random number generation functions\n",
        "    'sepal_width': np.random.normal(2.8, 0.31, n_per_class),\n",
        "    # np.random: NumPy random number generation functions\n",
        "    'petal_length': np.random.normal(4.3, 0.47, n_per_class),\n",
        "    # np.random: NumPy random number generation functions\n",
        "    'petal_width': np.random.normal(1.3, 0.20, n_per_class),\n",
        "    # np.random: NumPy random number generation functions\n",
        "    'species': 'versicolor'\n",
        "})\n",
        "\n",
        "# Virginica (larger flowers)\n",
        "virginica = pd.DataFrame({\n",
        "# pd.DataFrame: Creates a 2D labeled data structure (like a table/spreadsheet)\n",
        "    'sepal_length': np.random.normal(6.6, 0.64, n_per_class),\n",
        "    # np.random: NumPy random number generation functions\n",
        "    'sepal_width': np.random.normal(3.0, 0.32, n_per_class),\n",
        "    # np.random: NumPy random number generation functions\n",
        "    'petal_length': np.random.normal(5.5, 0.55, n_per_class),\n",
        "    # np.random: NumPy random number generation functions\n",
        "    'petal_width': np.random.normal(2.0, 0.27, n_per_class),\n",
        "    # np.random: NumPy random number generation functions\n",
        "    'species': 'virginica'\n",
        "})\n",
        "\n",
        "# Combine all\n",
        "df_iris = pd.concat([setosa, versicolor, virginica], ignore_index=True)\n",
        "# pd.concat(): Concatenate/stack DataFrames vertically (rows) or horizontally (columns)\n",
        "\n",
        "# Shuffle the dataset\n",
        "df_iris = df_iris.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "# .reset_index(): Reset index to default 0,1,2... numbering\n",
        "\n",
        "print(\"üå∏ Iris Dataset - Classic ML Dataset\")\n",
        "print(f\"Shape: {df_iris.shape}\")\n",
        "# .shape: Returns tuple (number_of_rows, number_of_columns)\n",
        "print(f\"\\nFirst 10 rows:\")\n",
        "print(df_iris.head(10))\n",
        "# .head(): Show the first N rows (default is 5 rows)\n",
        "print(f\"\\nSpecies distribution:\")\n",
        "print(df_iris['species'].value_counts())\n",
        "# .value_counts(): Count occurrences of each unique value, returns sorted Series\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.4 Essential DataFrame Operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Basic Info:\n",
            "   Rows: 150\n",
            "   Columns: 5\n",
            "   Memory: 14.39 KB\n",
            "\n",
            "2. Data Types:\n",
            "sepal_length    float64\n",
            "sepal_width     float64\n",
            "petal_length    float64\n",
            "petal_width     float64\n",
            "species          object\n",
            "dtype: object\n",
            "\n",
            "3. First and Last Rows:\n",
            "First 3:\n",
            "   sepal_length  sepal_width  petal_length  petal_width     species\n",
            "0          5.06         2.80          5.28         1.49  versicolor\n",
            "1          4.68         3.54          1.69         0.22      setosa\n",
            "2          7.35         2.83          5.55         1.49   virginica\n",
            "\n",
            "Last 3:\n",
            "     sepal_length  sepal_width  petal_length  petal_width     species\n",
            "147          4.40         3.71          1.47         0.36      setosa\n",
            "148          6.16         2.62          4.42         1.04  versicolor\n",
            "149          6.60         2.68          4.73         2.32   virginica\n",
            "\n",
            "4. Statistical Summary:\n",
            "       sepal_length  sepal_width  petal_length  petal_width\n",
            "count        150.00       150.00        150.00       150.00\n",
            "mean           5.82         3.06          3.72         1.20\n",
            "std            0.87         0.42          1.68         0.75\n",
            "min            4.31         1.80          1.17         0.08\n",
            "25%            5.08         2.75          1.59         0.32\n",
            "50%            5.73         3.05          4.29         1.32\n",
            "75%            6.45         3.35          5.04         1.88\n",
            "max            7.92         3.99          6.55         2.61\n"
          ]
        }
      ],
      "source": [
        "# Quick data inspection\n",
        "print(\"1. Basic Info:\")\n",
        "print(f\"   Rows: {len(df_iris)}\")\n",
        "print(f\"   Columns: {len(df_iris.columns)}\")\n",
        "# .columns: Get list of column names\n",
        "print(f\"   Memory: {df_iris.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
        "# .sum(): Calculate the total by adding all values together\n",
        "\n",
        "print(\"\\n2. Data Types:\")\n",
        "print(df_iris.dtypes)\n",
        "# .dtypes: Shows the data type of each column (int64, float64, object, etc.)\n",
        "\n",
        "print(\"\\n3. First and Last Rows:\")\n",
        "print(\"First 3:\")\n",
        "print(df_iris.head(3))\n",
        "# .head(): Show the first N rows (default is 5 rows)\n",
        "print(\"\\nLast 3:\")\n",
        "print(df_iris.tail(3))\n",
        "# .tail(): Show the last N rows (default is 5 rows)\n",
        "\n",
        "print(\"\\n4. Statistical Summary:\")\n",
        "print(df_iris.describe())\n",
        "# .describe(): Generate statistical summary (count, mean, std, min, quartiles, max)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detailed DataFrame Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 150 entries, 0 to 149\n",
            "Data columns (total 5 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   sepal_length  150 non-null    float64\n",
            " 1   sepal_width   150 non-null    float64\n",
            " 2   petal_length  150 non-null    float64\n",
            " 3   petal_width   150 non-null    float64\n",
            " 4   species       150 non-null    object \n",
            "dtypes: float64(4), object(1)\n",
            "memory usage: 6.0+ KB\n",
            "\n",
            "Column names:\n",
            "['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
            "\n",
            "Unique values per column:\n",
            "sepal_length: 150 unique values\n",
            "sepal_width: 150 unique values\n",
            "petal_length: 150 unique values\n",
            "petal_width: 150 unique values\n",
            "species: 3 unique values\n"
          ]
        }
      ],
      "source": [
        "# More detailed info\n",
        "print(\"Detailed DataFrame Info:\")\n",
        "df_iris.info()\n",
        "# .info(): Display DataFrame structure (columns, dtypes, non-null counts, memory)\n",
        "\n",
        "print(\"\\nColumn names:\")\n",
        "print(list(df_iris.columns))\n",
        "# .columns: Get list of column names\n",
        "\n",
        "print(\"\\nUnique values per column:\")\n",
        "for col in df_iris.columns:\n",
        "# .columns: Get list of column names\n",
        "    print(f\"{col}: {df_iris[col].nunique()} unique values\")\n",
        "    # .nunique(): Count number of unique/distinct values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.5 Selecting Data - Multiple Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method 1: Select single column (returns Series)\n",
        "print(\"Method 1: Single Column\")\n",
        "sepal_lengths = df_iris['sepal_length']\n",
        "print(sepal_lengths.head())\n",
        "# .head(): Show the first N rows (default is 5 rows)\n",
        "print(f\"Type: {type(sepal_lengths)}\")\n",
        "\n",
        "# Method 2: Select multiple columns (returns DataFrame)\n",
        "print(\"\\nMethod 2: Multiple Columns\")\n",
        "subset = df_iris[['sepal_length', 'petal_length', 'species']]\n",
        "print(subset.head())\n",
        "# .head(): Show the first N rows (default is 5 rows)\n",
        "\n",
        "# Method 3: Select by position (iloc)\n",
        "print(\"\\nMethod 3: By Position (iloc)\")\n",
        "print(\"First row:\")\n",
        "print(df_iris.iloc[0])\n",
        "# .iloc: Integer-location based indexing (access by row/column number, 0-indexed)\n",
        "print(\"\\nRows 5-8:\")\n",
        "print(df_iris.iloc[5:9])\n",
        "# .iloc: Integer-location based indexing (access by row/column number, 0-indexed)\n",
        "\n",
        "# Method 4: Select by label (loc)\n",
        "print(\"\\nMethod 4: By Label (loc)\")\n",
        "print(df_iris.loc[0:4, ['sepal_length', 'species']])\n",
        "# .loc: Label-based indexing (access by index/column names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.6 Data Filtering - Critical for ML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Single condition\n",
        "large_sepals = df_iris[df_iris['sepal_length'] > 6.5]\n",
        "print(f\"Flowers with sepal length > 6.5 cm: {len(large_sepals)}\")\n",
        "print(large_sepals.head())\n",
        "# .head(): Show the first N rows (default is 5 rows)\n",
        "\n",
        "# Multiple conditions with AND (&)\n",
        "print(\"\\nMultiple conditions (AND):\")\n",
        "filtered = df_iris[\n",
        "    (df_iris['sepal_length'] > 6.0) & \n",
        "    (df_iris['petal_length'] > 5.0)\n",
        "]\n",
        "print(f\"Large sepals AND large petals: {len(filtered)} flowers\")\n",
        "\n",
        "# Multiple conditions with OR (|)\n",
        "print(\"\\nMultiple conditions (OR):\")\n",
        "filtered_or = df_iris[\n",
        "    (df_iris['species'] == 'setosa') | \n",
        "    (df_iris['species'] == 'virginica')\n",
        "]\n",
        "print(f\"Setosa OR Virginica: {len(filtered_or)} flowers\")\n",
        "\n",
        "# Using isin() for multiple values\n",
        "species_filter = df_iris[df_iris['species'].isin(['setosa', 'virginica'])]\n",
        "# .isin(): Check if values are in a list (returns boolean mask)\n",
        "print(f\"\\nUsing isin(): {len(species_filter)} flowers\")\n",
        "\n",
        "# NOT condition\n",
        "not_setosa = df_iris[df_iris['species'] != 'setosa']\n",
        "print(f\"\\nNot setosa: {len(not_setosa)} flowers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# PART 2: DATA MANIPULATION & ANALYSIS\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Understanding GroupBy Operations (Theory)\n",
        "\n",
        "### üìä What is GroupBy?\n",
        "\n",
        "**GroupBy** is the process of splitting data into groups based on some criteria, applying a function to each group, and combining the results.\n",
        "\n",
        "```\n",
        "Split ‚Üí Apply ‚Üí Combine\n",
        "```\n",
        "\n",
        "### üéØ Why Use GroupBy?\n",
        "\n",
        "1. **Aggregation**\n",
        "   - Calculate statistics per group (mean, sum, count)\n",
        "   - Example: Average salary by department\n",
        "\n",
        "2. **Data Summarization**\n",
        "   - Understand patterns in subgroups\n",
        "   - Example: Sales by region, category\n",
        "\n",
        "3. **Feature Engineering for ML**\n",
        "   - Create group-based features\n",
        "   - Example: Customer's purchase vs. category average\n",
        "\n",
        "4. **Comparison Analysis**\n",
        "   - Compare different groups\n",
        "   - Example: Performance of different products\n",
        "\n",
        "### üìù When to Use GroupBy?\n",
        "\n",
        "‚úÖ **Use GroupBy when you need to:**\n",
        "- Answer questions like \"What is the average X by Y?\"\n",
        "- Aggregate data by categories\n",
        "- Find patterns within subgroups\n",
        "- Create summary statistics\n",
        "- Perform group-wise operations\n",
        "\n",
        "### üí° Real-World Examples\n",
        "\n",
        "```python\n",
        "# Business Questions that need GroupBy:\n",
        "\n",
        "1. \"What is the average salary by department?\"\n",
        "   ‚Üí df.groupby('Department')['Salary'].mean()\n",
        "\n",
        "2. \"How many sales per region?\"\n",
        "   ‚Üí df.groupby('Region')['Sales'].count()\n",
        "\n",
        "3. \"What is total revenue per product?\"\n",
        "   ‚Üí df.groupby('Product')['Revenue'].sum()\n",
        "\n",
        "4. \"Best performing salesperson in each region?\"\n",
        "   ‚Üí df.groupby('Region')['Sales'].max()\n",
        "```\n",
        "\n",
        "### üîç GroupBy Workflow\n",
        "\n",
        "```\n",
        "Original DataFrame:\n",
        "Product   Region   Sales\n",
        "Laptop    North    1000\n",
        "Mouse     North    200\n",
        "Laptop    South    1200\n",
        "Mouse     South    250\n",
        "\n",
        "After groupby('Product'):\n",
        "Group 1 (Laptop): [1000, 1200]\n",
        "Group 2 (Mouse):  [200, 250]\n",
        "\n",
        "After .mean():\n",
        "Laptop: 1100\n",
        "Mouse:  225\n",
        "```\n",
        "\n",
        "### üí™ Common Aggregation Functions\n",
        "\n",
        "| Function | Description | Use Case |\n",
        "|----------|-------------|----------|\n",
        "| `.mean()` | Average | Average price per category |\n",
        "| `.sum()` | Total | Total sales per region |\n",
        "| `.count()` | Number of items | Orders per customer |\n",
        "| `.min()` | Minimum value | Lowest price per product |\n",
        "| `.max()` | Maximum value | Highest salary per dept |\n",
        "| `.std()` | Standard deviation | Price variation |\n",
        "| `.agg()` | Multiple functions | All stats at once |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 GroupBy - Practical Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean measurements by species:\n",
            "            sepal_length  sepal_width  petal_length  petal_width\n",
            "species                                                         \n",
            "setosa              4.92         3.41          1.49         0.26\n",
            "versicolor          5.98         2.79          4.32         1.34\n",
            "virginica           6.57         2.98          5.36         2.01\n",
            "\n",
            "Standard deviation by species:\n",
            "            sepal_length  petal_length\n",
            "species                               \n",
            "setosa              0.33          0.17\n",
            "versicolor          0.57          0.36\n",
            "virginica           0.69          0.50\n"
          ]
        }
      ],
      "source": [
        "# Basic groupby\n",
        "print(\"Mean measurements by species:\")\n",
        "species_means = df_iris.groupby('species')[[\n",
        "# .groupby(): Split data into groups based on column values for aggregation\n",
        "    'sepal_length', 'sepal_width', 'petal_length', 'petal_width'\n",
        "]].mean()\n",
        "# .mean(): Calculate the average value (sum divided by count)\n",
        "print(species_means)\n",
        "\n",
        "print(\"\\nStandard deviation by species:\")\n",
        "species_std = df_iris.groupby('species')[[\n",
        "# .groupby(): Split data into groups based on column values for aggregation\n",
        "    'sepal_length', 'petal_length'\n",
        "]].std()\n",
        "# .std(): Calculate standard deviation (measure of spread/variability)\n",
        "print(species_std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Multiple aggregations for petal_length:\n",
            "            count  mean   std   min   max\n",
            "species                                  \n",
            "setosa         50  1.49  0.17  1.17  1.92\n",
            "versicolor     50  4.32  0.36  3.62  5.28\n",
            "virginica      50  5.36  0.50  4.14  6.55\n",
            "\n",
            "Named aggregations:\n",
            "            avg_sepal_length  max_petal_length  total_count\n",
            "species                                                    \n",
            "setosa                  4.92              1.92           50\n",
            "versicolor              5.98              5.28           50\n",
            "virginica               6.57              6.55           50\n"
          ]
        }
      ],
      "source": [
        "# Multiple aggregations\n",
        "print(\"Multiple aggregations for petal_length:\")\n",
        "multi_agg = df_iris.groupby('species')['petal_length'].agg([\n",
        "# .groupby(): Split data into groups based on column values for aggregation\n",
        "    'count', 'mean', 'std', 'min', 'max'\n",
        "])\n",
        "print(multi_agg)\n",
        "\n",
        "# Named aggregations\n",
        "print(\"\\nNamed aggregations:\")\n",
        "named_agg = df_iris.groupby('species').agg(\n",
        "# .groupby(): Split data into groups based on column values for aggregation\n",
        "    avg_sepal_length=('sepal_length', 'mean'),\n",
        "    max_petal_length=('petal_length', 'max'),\n",
        "    total_count=('species', 'count')\n",
        ")\n",
        "print(named_agg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Creating Wine Quality Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üç∑ Wine Quality Dataset\n",
            "Shape: (200, 8)\n",
            "\n",
            "First rows:\n",
            "   alcohol  acidity    pH  density  sulphates  residual_sugar  quality wine_type\n",
            "0    10.62     3.46  2.90     0.99       1.15            3.59        7      Rose\n",
            "1    14.66     2.63  3.70     0.99       0.48            8.59        7       Red\n",
            "2    13.12     2.74  3.31     0.99       0.99           13.22        7      Rose\n",
            "3    12.19     3.85  3.63     0.99       1.03           11.25        8       Red\n",
            "4     9.09     3.41  3.12     0.99       0.81           12.29        3     White\n",
            "\n",
            "Quality distribution:\n",
            "3    20\n",
            "4    22\n",
            "5    40\n",
            "6    43\n",
            "7    37\n",
            "8    26\n",
            "9    12\n",
            "Name: quality, dtype: int64\n",
            "\n",
            "Wine type distribution:\n",
            "Red      98\n",
            "White    85\n",
            "Rose     17\n",
            "Name: wine_type, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Create wine quality dataset\n",
        "np.random.seed(42)\n",
        "# np.random: NumPy random number generation functions\n",
        "n_wines = 200\n",
        "\n",
        "df_wine = pd.DataFrame({\n",
        "# pd.DataFrame: Creates a 2D labeled data structure (like a table/spreadsheet)\n",
        "    'alcohol': np.random.uniform(8, 15, n_wines),\n",
        "    # np.random: NumPy random number generation functions\n",
        "    'acidity': np.random.uniform(2.5, 4.0, n_wines),\n",
        "    # np.random: NumPy random number generation functions\n",
        "    'pH': np.random.uniform(2.8, 3.8, n_wines),\n",
        "    # np.random: NumPy random number generation functions\n",
        "    'density': np.random.uniform(0.99, 1.00, n_wines),\n",
        "    # np.random: NumPy random number generation functions\n",
        "    'sulphates': np.random.uniform(0.3, 1.5, n_wines),\n",
        "    # np.random: NumPy random number generation functions\n",
        "    'residual_sugar': np.random.uniform(1, 15, n_wines),\n",
        "    # np.random: NumPy random number generation functions\n",
        "})\n",
        "\n",
        "# Create quality score based on features\n",
        "df_wine['quality'] = (\n",
        "    (df_wine['alcohol'] * 0.5) + \n",
        "    (df_wine['sulphates'] * 2) - \n",
        "    (df_wine['acidity'] * 0.5) + \n",
        "    np.random.normal(0, 1, n_wines)\n",
        "    # np.random: NumPy random number generation functions\n",
        ").round(0).astype(int)\n",
        "# .round(): Round values to specified number of decimal places\n",
        "df_wine['quality'] = df_wine['quality'].clip(3, 9)\n",
        "# .clip(): Limit values to specified min/max range\n",
        "\n",
        "# Create wine type\n",
        "df_wine['wine_type'] = np.random.choice(['Red', 'White', 'Rose'], n_wines, p=[0.5, 0.4, 0.1])\n",
        "# np.random: NumPy random number generation functions\n",
        "\n",
        "print(\"üç∑ Wine Quality Dataset\")\n",
        "print(f\"Shape: {df_wine.shape}\")\n",
        "# .shape: Returns tuple (number_of_rows, number_of_columns)\n",
        "print(\"\\nFirst rows:\")\n",
        "print(df_wine.head())\n",
        "# .head(): Show the first N rows (default is 5 rows)\n",
        "\n",
        "print(\"\\nQuality distribution:\")\n",
        "print(df_wine['quality'].value_counts().sort_index())\n",
        "# .value_counts(): Count occurrences of each unique value, returns sorted Series\n",
        "\n",
        "print(\"\\nWine type distribution:\")\n",
        "print(df_wine['wine_type'].value_counts())\n",
        "# .value_counts(): Count occurrences of each unique value, returns sorted Series\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.4 Understanding Correlation Analysis (Theory)\n",
        "\n",
        "### üìä What is Correlation?\n",
        "\n",
        "**Correlation** measures the strength and direction of the relationship between two variables.\n",
        "\n",
        "```\n",
        "Correlation Coefficient (r) ranges from -1 to +1:\n",
        "\n",
        "+1.0 : Perfect positive correlation (both increase together)\n",
        " 0.0 : No correlation (independent variables)\n",
        "-1.0 : Perfect negative correlation (one increases, other decreases)\n",
        "```\n",
        "\n",
        "### üéØ Why Use Correlation in ML?\n",
        "\n",
        "1. **Feature Selection**\n",
        "   - Identify which features correlate with target variable\n",
        "   - Remove weakly correlated features\n",
        "   - Focus on important features\n",
        "\n",
        "2. **Detect Redundant Features**\n",
        "   - Find highly correlated features (multicollinearity)\n",
        "   - Keep only one from highly correlated pairs\n",
        "   - Reduce dimensionality\n",
        "\n",
        "3. **Understand Relationships**\n",
        "   - Discover patterns in data\n",
        "   - Validate domain knowledge\n",
        "   - Guide feature engineering\n",
        "\n",
        "4. **Improve Model Performance**\n",
        "   - Remove redundant features ‚Üí faster training\n",
        "   - Keep relevant features ‚Üí better accuracy\n",
        "   - Avoid overfitting\n",
        "\n",
        "### üìù Interpreting Correlation Values\n",
        "\n",
        "| Correlation (|r|) | Interpretation | Action for ML |\n",
        "|-------------------|----------------|----------------|\n",
        "| 0.9 - 1.0 | Very strong | Keep if with target, remove if between features |\n",
        "| 0.7 - 0.9 | Strong | Important features |\n",
        "| 0.5 - 0.7 | Moderate | Consider keeping |\n",
        "| 0.3 - 0.5 | Weak | May or may not be useful |\n",
        "| 0.0 - 0.3 | Very weak | Consider removing |\n",
        "\n",
        "### üí° Common Patterns\n",
        "\n",
        "```python\n",
        "# With Target Variable:\n",
        "High Correlation (0.7+)  ‚Üí Keep feature (important predictor)\n",
        "Low Correlation (< 0.3)  ‚Üí Consider removing (not useful)\n",
        "\n",
        "# Between Features:\n",
        "High Correlation (0.8+)  ‚Üí Redundant! Keep only one\n",
        "Low Correlation          ‚Üí Good! Independent information\n",
        "```\n",
        "\n",
        "### üéì Real-World Example\n",
        "\n",
        "```\n",
        "Housing Price Prediction:\n",
        "\n",
        "High correlation with price:\n",
        "  ‚úÖ Square footage: 0.85  ‚Üí Keep (important!)\n",
        "  ‚úÖ Number of rooms: 0.72 ‚Üí Keep (important!)\n",
        "\n",
        "Low correlation with price:\n",
        "  ‚ùå Year built: 0.12      ‚Üí Consider removing\n",
        "  ‚ùå Paint color: 0.02     ‚Üí Remove (not useful)\n",
        "\n",
        "High correlation with each other:\n",
        "  ‚ö†Ô∏è  Rooms & Bathrooms: 0.92 ‚Üí Keep only one (redundant)\n",
        "```\n",
        "\n",
        "### ‚ö†Ô∏è Important Warnings\n",
        "\n",
        "1. **Correlation ‚â† Causation**\n",
        "   - High correlation doesn't mean one causes the other\n",
        "   - Example: Ice cream sales & drownings (both caused by summer)\n",
        "\n",
        "2. **Linear Relationships Only**\n",
        "   - Pearson correlation only measures linear relationships\n",
        "   - Non-linear relationships might show low correlation\n",
        "\n",
        "3. **Outliers Impact**\n",
        "   - Outliers can significantly affect correlation\n",
        "   - Always check for outliers first\n",
        "\n",
        "### üîç When to Use Correlation?\n",
        "\n",
        "‚úÖ **Use Correlation for:**\n",
        "- Initial feature selection\n",
        "- Identifying redundant features\n",
        "- Quick exploratory data analysis\n",
        "- Linear regression problems\n",
        "\n",
        "‚ùå **Don't rely solely on Correlation for:**\n",
        "- Non-linear relationships\n",
        "- Categorical variables (use other methods)\n",
        "- Complex interactions between features\n",
        "- Final feature selection (use model-based methods too)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.5 Correlation Analysis - Practical Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correlation Matrix for Iris:\n",
            "              sepal_length  sepal_width  petal_length  petal_width\n",
            "sepal_length          1.00        -0.32          0.75         0.73\n",
            "sepal_width          -0.32         1.00         -0.51        -0.47\n",
            "petal_length          0.75        -0.51          1.00         0.94\n",
            "petal_width           0.73        -0.47          0.94         1.00\n",
            "\n",
            "üîç Highly Correlated Feature Pairs (|correlation| > 0.8):\n",
            "  petal_length <-> petal_width: 0.937\n"
          ]
        }
      ],
      "source": [
        "# Calculate correlation matrix\n",
        "numeric_cols = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
        "correlation_matrix = df_iris[numeric_cols].corr()\n",
        "# .corr(): Calculate pairwise correlation between columns (-1 to +1)\n",
        "\n",
        "print(\"Correlation Matrix for Iris:\")\n",
        "print(correlation_matrix)\n",
        "\n",
        "# Find highly correlated features (|corr| > 0.8)\n",
        "print(\"\\nüîç Highly Correlated Feature Pairs (|correlation| > 0.8):\")\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "# .columns: Get list of column names\n",
        "    for j in range(i+1, len(correlation_matrix.columns)):\n",
        "    # .columns: Get list of column names\n",
        "        corr_value = correlation_matrix.iloc[i, j]\n",
        "        # .iloc: Integer-location based indexing (access by row/column number, 0-indexed)\n",
        "        if abs(corr_value) > 0.8:\n",
        "            col1 = correlation_matrix.columns[i]\n",
        "            # .columns: Get list of column names\n",
        "            col2 = correlation_matrix.columns[j]\n",
        "            # .columns: Get list of column names\n",
        "            print(f\"  {col1} <-> {col2}: {corr_value:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wine Features Correlation with Quality:\n",
            "quality           1.00\n",
            "alcohol           0.64\n",
            "sulphates         0.46\n",
            "residual_sugar    0.02\n",
            "density          -0.02\n",
            "pH               -0.10\n",
            "acidity          -0.17\n",
            "Name: quality, dtype: float64\n",
            "\n",
            "üìä Top 3 features most correlated with quality:\n",
            "alcohol           0.64\n",
            "sulphates         0.46\n",
            "residual_sugar    0.02\n",
            "Name: quality, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Correlation with specific target\n",
        "# For wine dataset, correlate all features with quality\n",
        "wine_correlations = df_wine.select_dtypes(include=[np.number]).corr()['quality'].sort_values(ascending=False)\n",
        "# .sort_values(): Sort DataFrame by values in specified column(s)\n",
        "\n",
        "print(\"Wine Features Correlation with Quality:\")\n",
        "print(wine_correlations)\n",
        "\n",
        "print(\"\\nüìä Top 3 features most correlated with quality:\")\n",
        "print(wine_correlations[1:4])  # Skip quality itself"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.6 Understanding Missing Data (Theory)\n",
        "\n",
        "### üìä What is Missing Data?\n",
        "\n",
        "**Missing Data** refers to values that are not stored/available for a variable in an observation.\n",
        "\n",
        "```\n",
        "In pandas, missing values are represented as:\n",
        "- NaN (Not a Number) for numeric data\n",
        "- None for object/string data\n",
        "- NaT (Not a Time) for datetime data\n",
        "```\n",
        "\n",
        "### üéØ Why Do We Have Missing Data?\n",
        "\n",
        "1. **Data Collection Issues**\n",
        "   - Sensor malfunction\n",
        "   - Survey questions skipped\n",
        "   - Forms not completely filled\n",
        "\n",
        "2. **Data Processing Errors**\n",
        "   - Merge/join operations\n",
        "   - Data transformation errors\n",
        "   - File corruption\n",
        "\n",
        "3. **Intentional Omissions**\n",
        "   - Not applicable (N/A)\n",
        "   - Privacy concerns\n",
        "   - Optional fields\n",
        "\n",
        "### ‚ö†Ô∏è Why is Missing Data a Problem for ML?\n",
        "\n",
        "1. **Most ML Algorithms Can't Handle NaN**\n",
        "   ```python\n",
        "   model.fit(X_train)  # ‚ùå Error if X_train has NaN\n",
        "   ```\n",
        "\n",
        "2. **Reduces Dataset Size**\n",
        "   - Dropping rows with missing values loses information\n",
        "   - Smaller dataset ‚Üí worse model performance\n",
        "\n",
        "3. **Biased Results**\n",
        "   - Missing data might not be random\n",
        "   - Can introduce systematic bias\n",
        "\n",
        "4. **Impacts Statistical Analysis**\n",
        "   - Incorrect mean, std, correlations\n",
        "   - Invalid conclusions\n",
        "\n",
        "### üìù Types of Missing Data\n",
        "\n",
        "#### **1. MCAR (Missing Completely At Random)**\n",
        "```\n",
        "Probability of being missing is same for all observations\n",
        "Example: Random sensor failure\n",
        "Solution: Safe to delete or impute\n",
        "```\n",
        "\n",
        "#### **2. MAR (Missing At Random)**\n",
        "```\n",
        "Probability depends on other observed variables\n",
        "Example: Younger people skip income question more\n",
        "Solution: Impute based on other variables\n",
        "```\n",
        "\n",
        "#### **3. MNAR (Missing Not At Random)**\n",
        "```\n",
        "Probability depends on the missing value itself\n",
        "Example: High earners don't report income\n",
        "Solution: Complex, may need domain knowledge\n",
        "```\n",
        "\n",
        "### üõ†Ô∏è Strategies to Handle Missing Data\n",
        "\n",
        "#### **Strategy 1: Deletion**\n",
        "\n",
        "**A. Drop Rows (Listwise Deletion)**\n",
        "```python\n",
        "df.dropna()  # Remove any row with missing value\n",
        "```\n",
        "\n",
        "**When to use:**\n",
        "- ‚úÖ Very few missing values (< 5%)\n",
        "- ‚úÖ Large dataset (can afford to lose rows)\n",
        "- ‚úÖ Missing data is MCAR\n",
        "\n",
        "**Pros:** Simple, no assumptions\n",
        "**Cons:** Lose data, potential bias\n",
        "\n",
        "**B. Drop Columns**\n",
        "```python\n",
        "df.dropna(axis=1)  # Remove columns with missing values\n",
        "```\n",
        "\n",
        "**When to use:**\n",
        "- ‚úÖ Column has > 50% missing values\n",
        "- ‚úÖ Feature is not important\n",
        "\n",
        "**Pros:** Keep all rows\n",
        "**Cons:** Lose potentially useful feature\n",
        "\n",
        "#### **Strategy 2: Imputation (Filling)**\n",
        "\n",
        "**A. Mean Imputation**\n",
        "```python\n",
        "df['col'].fillna(df['col'].mean())\n",
        "```\n",
        "\n",
        "**When to use:**\n",
        "- ‚úÖ Numeric data\n",
        "- ‚úÖ Data is normally distributed\n",
        "- ‚úÖ Moderate missing values (5-20%)\n",
        "\n",
        "**Pros:** Simple, maintains mean\n",
        "**Cons:** Reduces variance, ignores relationships\n",
        "\n",
        "**B. Median Imputation**\n",
        "```python\n",
        "df['col'].fillna(df['col'].median())\n",
        "```\n",
        "\n",
        "**When to use:**\n",
        "- ‚úÖ Numeric data with outliers\n",
        "- ‚úÖ Skewed distributions\n",
        "\n",
        "**Pros:** Robust to outliers\n",
        "**Cons:** Same as mean imputation\n",
        "\n",
        "**C. Mode Imputation**\n",
        "```python\n",
        "df['col'].fillna(df['col'].mode()[0])\n",
        "```\n",
        "\n",
        "**When to use:**\n",
        "- ‚úÖ Categorical data\n",
        "- ‚úÖ Small number of missing values\n",
        "\n",
        "**Pros:** Preserves most frequent value\n",
        "**Cons:** Can create bias toward majority class\n",
        "\n",
        "**D. Forward/Backward Fill**\n",
        "```python\n",
        "df['col'].fillna(method='ffill')  # Use previous value\n",
        "df['col'].fillna(method='bfill')  # Use next value\n",
        "```\n",
        "\n",
        "**When to use:**\n",
        "- ‚úÖ Time series data\n",
        "- ‚úÖ Sequential/ordered data\n",
        "\n",
        "**Pros:** Maintains temporal patterns\n",
        "**Cons:** Can propagate errors\n",
        "\n",
        "**E. Constant Value**\n",
        "```python\n",
        "df['col'].fillna(0)  or  df['col'].fillna('Unknown')\n",
        "```\n",
        "\n",
        "**When to use:**\n",
        "- ‚úÖ Missing means \"none\" or \"zero\"\n",
        "- ‚úÖ Creating \"missing\" as a separate category\n",
        "\n",
        "**Pros:** Explicit handling\n",
        "**Cons:** May not be meaningful\n",
        "\n",
        "### üéØ Decision Framework\n",
        "\n",
        "```\n",
        "Amount of Missing Data?\n",
        "    ‚Üì\n",
        "< 5%: Drop rows\n",
        "    ‚Üì\n",
        "5-20%: Impute (mean/median/mode)\n",
        "    ‚Üì\n",
        "20-50%: Advanced imputation (KNN, models)\n",
        "    ‚Üì\n",
        "> 50%: Drop column or create \"missing\" indicator\n",
        "```\n",
        "\n",
        "### üìä Best Practices\n",
        "\n",
        "1. **Analyze Pattern First**\n",
        "   ```python\n",
        "   df.isnull().sum()  # Count missing per column\n",
        "   df.isnull().sum() / len(df)  # Percentage missing\n",
        "   ```\n",
        "\n",
        "2. **Document Your Decision**\n",
        "   - Why did you drop/impute?\n",
        "   - What impact on results?\n",
        "\n",
        "3. **Create Missing Indicator**\n",
        "   ```python\n",
        "   df['col_missing'] = df['col'].isnull().astype(int)\n",
        "   # Then impute the original column\n",
        "   # Model can learn if \"missingness\" is informative\n",
        "   ```\n",
        "\n",
        "4. **Different Strategies for Different Columns**\n",
        "   ```python\n",
        "   df['numeric_col'].fillna(df['numeric_col'].median())\n",
        "   df['category_col'].fillna(df['category_col'].mode()[0])\n",
        "   ```\n",
        "\n",
        "### ‚ö†Ô∏è Common Mistakes\n",
        "\n",
        "‚ùå **Mistake 1**: Imputing before train-test split\n",
        "```python\n",
        "# WRONG:\n",
        "df['col'].fillna(df['col'].mean())  # Uses ALL data\n",
        "train, test = split(df)\n",
        "\n",
        "# CORRECT:\n",
        "train, test = split(df)\n",
        "mean_train = train['col'].mean()\n",
        "train['col'].fillna(mean_train)\n",
        "test['col'].fillna(mean_train)  # Use training mean\n",
        "```\n",
        "\n",
        "‚ùå **Mistake 2**: Using mean for categorical data\n",
        "```python\n",
        "# WRONG:\n",
        "df['City'].fillna(df['City'].mean())  # ‚ùå Can't average cities!\n",
        "\n",
        "# CORRECT:\n",
        "df['City'].fillna(df['City'].mode()[0])  # ‚úÖ Use most common\n",
        "```\n",
        "\n",
        "‚ùå **Mistake 3**: Ignoring missing data type\n",
        "```python\n",
        "# Always investigate WHY data is missing\n",
        "# Different missing patterns need different handling\n",
        "```\n",
        "\n",
        "### üí° Summary Table\n",
        "\n",
        "| Data Type | Recommended | Alternative |\n",
        "|-----------|-------------|-------------|\n",
        "| Numeric (normal) | Mean | Median |\n",
        "| Numeric (skewed) | Median | Mean |\n",
        "| Categorical | Mode | Create \"Unknown\" |\n",
        "| Time Series | Forward Fill | Interpolation |\n",
        "| Boolean | Mode | False/True |\n",
        "| High % missing | Drop column | Create indicator |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.7 Handling Missing Data - Practical Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset with missing values:\n",
            "   Student  Math_Score  Science_Score  English_Score Grade\n",
            "0    Alice        85.5           90.0           88.0     A\n",
            "1      Bob         NaN           88.5           92.0     B\n",
            "2  Charlie        78.0            NaN           85.0     A\n",
            "3    David        92.5           95.0            NaN     A\n",
            "4      Eve         NaN           87.5           90.5     B\n",
            "5    Frank        88.0            NaN           86.5   NaN\n",
            "6    Grace        91.5           89.0           94.0     A\n",
            "7    Henry        82.0           93.5           87.5     B\n",
            "\n",
            "üìä Missing Value Analysis:\n",
            "Missing values per column:\n",
            "Student          0\n",
            "Math_Score       2\n",
            "Science_Score    2\n",
            "English_Score    1\n",
            "Grade            1\n",
            "dtype: int64\n",
            "\n",
            "Total missing: 6\n",
            "Percentage missing: 15.0%\n",
            "\n",
            "Rows with any missing values:\n",
            "   Student  Math_Score  Science_Score  English_Score Grade\n",
            "1      Bob         NaN           88.5           92.0     B\n",
            "2  Charlie        78.0            NaN           85.0     A\n",
            "3    David        92.5           95.0            NaN     A\n",
            "4      Eve         NaN           87.5           90.5     B\n",
            "5    Frank        88.0            NaN           86.5   NaN\n"
          ]
        }
      ],
      "source": [
        "# Create dataset with missing values\n",
        "data_missing = {\n",
        "    'Student': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank', 'Grace', 'Henry'],\n",
        "    'Math_Score': [85.5, np.nan, 78.0, 92.5, np.nan, 88.0, 91.5, 82.0],\n",
        "    'Science_Score': [90.0, 88.5, np.nan, 95.0, 87.5, np.nan, 89.0, 93.5],\n",
        "    'English_Score': [88.0, 92.0, 85.0, np.nan, 90.5, 86.5, 94.0, 87.5],\n",
        "    'Grade': ['A', 'B', 'A', 'A', 'B', np.nan, 'A', 'B']\n",
        "}\n",
        "\n",
        "df_missing = pd.DataFrame(data_missing)\n",
        "# pd.DataFrame: Creates a 2D labeled data structure (like a table/spreadsheet)\n",
        "print(\"Dataset with missing values:\")\n",
        "print(df_missing)\n",
        "\n",
        "print(\"\\nüìä Missing Value Analysis:\")\n",
        "print(\"Missing values per column:\")\n",
        "print(df_missing.isnull().sum())\n",
        "# .sum(): Calculate the total by adding all values together\n",
        "\n",
        "print(f\"\\nTotal missing: {df_missing.isnull().sum().sum()}\")\n",
        "# .sum(): Calculate the total by adding all values together\n",
        "total_cells = df_missing.shape[0] * df_missing.shape[1]\n",
        "# .shape: Returns tuple (number_of_rows, number_of_columns)\n",
        "missing_pct = (df_missing.isnull().sum().sum() / total_cells) * 100\n",
        "# .sum(): Calculate the total by adding all values together\n",
        "print(f\"Percentage missing: {missing_pct:.1f}%\")\n",
        "\n",
        "# Check which rows have missing values\n",
        "print(\"\\nRows with any missing values:\")\n",
        "print(df_missing[df_missing.isnull().any(axis=1)])\n",
        "# axis=1: Operate along columns (ACROSS the rows), applies function to each row\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Strategy 1: Drop rows with any missing values\n",
        "print(\"Strategy 1: Drop rows with ANY missing values\")\n",
        "df_dropped = df_missing.dropna()\n",
        "# .dropna(): Remove rows or columns containing NaN (missing values)\n",
        "print(f\"Original rows: {len(df_missing)}\")\n",
        "print(f\"After dropping: {len(df_dropped)}\")\n",
        "print(df_dropped)\n",
        "\n",
        "# Strategy 2: Drop rows with missing in specific columns\n",
        "print(\"\\nStrategy 2: Drop only if Math_Score is missing\")\n",
        "df_dropped_subset = df_missing.dropna(subset=['Math_Score'])\n",
        "# .dropna(): Remove rows or columns containing NaN (missing values)\n",
        "print(f\"After dropping: {len(df_dropped_subset)} rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Strategy 3: Fill with mean (for numeric columns)\n",
        "print(\"Strategy 3: Fill numeric columns with MEAN\")\n",
        "df_filled_mean = df_missing.copy()\n",
        "df_filled_mean['Math_Score'].fillna(df_filled_mean['Math_Score'].mean(), inplace=True)\n",
        "# .mean(): Calculate the average value (sum divided by count)\n",
        "df_filled_mean['Science_Score'].fillna(df_filled_mean['Science_Score'].mean(), inplace=True)\n",
        "# .mean(): Calculate the average value (sum divided by count)\n",
        "df_filled_mean['English_Score'].fillna(df_filled_mean['English_Score'].mean(), inplace=True)\n",
        "# .mean(): Calculate the average value (sum divided by count)\n",
        "print(df_filled_mean)\n",
        "\n",
        "# Strategy 4: Fill with median (more robust to outliers)\n",
        "print(\"\\nStrategy 4: Fill numeric columns with MEDIAN\")\n",
        "df_filled_median = df_missing.copy()\n",
        "for col in ['Math_Score', 'Science_Score', 'English_Score']:\n",
        "    df_filled_median[col].fillna(df_filled_median[col].median(), inplace=True)\n",
        "    # .fillna(): Replace NaN (missing values) with specified value\n",
        "print(df_filled_median)\n",
        "\n",
        "# Strategy 5: Fill categorical with mode\n",
        "print(\"\\nStrategy 5: Fill categorical with MODE\")\n",
        "df_filled_mode = df_missing.copy()\n",
        "mode_grade = df_filled_mode['Grade'].mode()[0]\n",
        "df_filled_mode['Grade'].fillna(mode_grade, inplace=True)\n",
        "# .fillna(): Replace NaN (missing values) with specified value\n",
        "print(f\"Mode of Grade: {mode_grade}\")\n",
        "print(df_filled_mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced: Forward fill and backward fill\n",
        "print(\"Strategy 6: Forward Fill (use previous value)\")\n",
        "df_ffill = df_missing.copy()\n",
        "df_ffill['Math_Score'].fillna(method='ffill', inplace=True)\n",
        "# .fillna(): Replace NaN (missing values) with specified value\n",
        "print(df_ffill[['Student', 'Math_Score']])\n",
        "\n",
        "print(\"\\nStrategy 7: Backward Fill (use next value)\")\n",
        "df_bfill = df_missing.copy()\n",
        "df_bfill['Math_Score'].fillna(method='bfill', inplace=True)\n",
        "# .fillna(): Replace NaN (missing values) with specified value\n",
        "print(df_bfill[['Student', 'Math_Score']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.5 Sorting and Ranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sort iris by sepal_length (ascending):\n",
            "     sepal_length species\n",
            "132          4.31  setosa\n",
            "97           4.33  setosa\n",
            "81           4.38  setosa\n",
            "147          4.40  setosa\n",
            "58           4.48  setosa\n",
            "\n",
            "Sort by sepal_length (descending):\n",
            "     sepal_length     species\n",
            "91           7.92   virginica\n",
            "119          7.90  versicolor\n",
            "142          7.72   virginica\n",
            "104          7.64   virginica\n",
            "139          7.59   virginica\n",
            "\n",
            "Sort by species then by sepal_length:\n",
            "    species  sepal_length  petal_length\n",
            "5    setosa          5.65          1.51\n",
            "105  setosa          5.55          1.82\n",
            "100  setosa          5.53          1.36\n",
            "144  setosa          5.51          1.63\n",
            "68   setosa          5.37          1.28\n",
            "85   setosa          5.29          1.34\n",
            "87   setosa          5.27          1.53\n",
            "57   setosa          5.26          1.54\n",
            "108  setosa          5.23          1.44\n",
            "13   setosa          5.19          1.49\n"
          ]
        }
      ],
      "source": [
        "# Sort by single column\n",
        "print(\"Sort iris by sepal_length (ascending):\")\n",
        "sorted_asc = df_iris.sort_values('sepal_length').head()\n",
        "# .head(): Show the first N rows (default is 5 rows)\n",
        "print(sorted_asc[['sepal_length', 'species']])\n",
        "\n",
        "# Sort descending\n",
        "print(\"\\nSort by sepal_length (descending):\")\n",
        "sorted_desc = df_iris.sort_values('sepal_length', ascending=False).head()\n",
        "# .head(): Show the first N rows (default is 5 rows)\n",
        "print(sorted_desc[['sepal_length', 'species']])\n",
        "\n",
        "# Sort by multiple columns\n",
        "print(\"\\nSort by species then by sepal_length:\")\n",
        "sorted_multi = df_iris.sort_values(['species', 'sepal_length'], ascending=[True, False]).head(10)\n",
        "# .head(): Show the first N rows (default is 5 rows)\n",
        "print(sorted_multi[['species', 'sepal_length', 'petal_length']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 flowers by sepal length:\n",
            "     sepal_length  sepal_length_rank     species\n",
            "91           7.92                1.0   virginica\n",
            "119          7.90                2.0  versicolor\n",
            "142          7.72                3.0   virginica\n",
            "104          7.64                4.0   virginica\n",
            "139          7.59                5.0   virginica\n",
            "93           7.56                6.0   virginica\n",
            "99           7.48                7.0   virginica\n",
            "70           7.42                8.0   virginica\n",
            "2            7.35                9.0   virginica\n",
            "23           7.28               10.0   virginica\n"
          ]
        }
      ],
      "source": [
        "# Ranking\n",
        "df_iris['sepal_length_rank'] = df_iris['sepal_length'].rank(ascending=False)\n",
        "# ascending=False: Sort in descending order (largest to smallest, Z to A)\n",
        "print(\"Top 10 flowers by sepal length:\")\n",
        "top_10 = df_iris.nsmallest(10, 'sepal_length_rank')\n",
        "# .nsmallest(): Return N smallest values by specified column\n",
        "print(top_10[['sepal_length', 'sepal_length_rank', 'species']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.6 Merging and Joining Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Customers:\n",
            "   CustomerID     Name     City\n",
            "0           1    Alice      NYC\n",
            "1           2      Bob       LA\n",
            "2           3  Charlie  Chicago\n",
            "3           4    David  Houston\n",
            "4           5      Eve  Phoenix\n",
            "\n",
            "Orders:\n",
            "   OrderID  CustomerID  Amount   Product\n",
            "0      101           1     250    Laptop\n",
            "1      102           2     450     Phone\n",
            "2      103           1     180     Mouse\n",
            "3      104           3     320  Keyboard\n",
            "4      105           5     200   Monitor\n",
            "5      106           6     150    Tablet\n"
          ]
        }
      ],
      "source": [
        "# Create related datasets\n",
        "customers = pd.DataFrame({\n",
        "# pd.DataFrame: Creates a 2D labeled data structure (like a table/spreadsheet)\n",
        "    'CustomerID': [1, 2, 3, 4, 5],\n",
        "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
        "    'City': ['NYC', 'LA', 'Chicago', 'Houston', 'Phoenix']\n",
        "})\n",
        "\n",
        "orders = pd.DataFrame({\n",
        "# pd.DataFrame: Creates a 2D labeled data structure (like a table/spreadsheet)\n",
        "    'OrderID': [101, 102, 103, 104, 105, 106],\n",
        "    'CustomerID': [1, 2, 1, 3, 5, 6],\n",
        "    'Amount': [250, 450, 180, 320, 200, 150],\n",
        "    'Product': ['Laptop', 'Phone', 'Mouse', 'Keyboard', 'Monitor', 'Tablet']\n",
        "})\n",
        "\n",
        "print(\"Customers:\")\n",
        "print(customers)\n",
        "print(\"\\nOrders:\")\n",
        "print(orders)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inner join (only matching records)\n",
        "print(\"Inner Join (only matching CustomerIDs):\")\n",
        "inner_merged = pd.merge(customers, orders, on='CustomerID', how='inner')  # INNER JOIN: Keep only matching rows (intersection)\n",
        "print(inner_merged)\n",
        "print(f\"\\nRows in result: {len(inner_merged)}\")\n",
        "\n",
        "# Left join (all customers, matching orders)\n",
        "print(\"\\nLeft Join (all customers):\")\n",
        "left_merged = pd.merge(customers, orders, on='CustomerID', how='left')  # LEFT JOIN: Keep all left rows, match right (NULL if no match)\n",
        "print(left_merged)\n",
        "\n",
        "# Right join (all orders, matching customers)\n",
        "print(\"\\nRight Join (all orders):\")\n",
        "right_merged = pd.merge(customers, orders, on='CustomerID', how='right')  # RIGHT JOIN: Keep all right rows, match left (NULL if no match)\n",
        "print(right_merged)\n",
        "\n",
        "# Outer join (all records from both)\n",
        "print(\"\\nOuter Join (all records):\")\n",
        "outer_merged = pd.merge(customers, orders, on='CustomerID', how='outer')  # OUTER JOIN: Keep all rows from both (UNION, NULL where no match)\n",
        "print(outer_merged)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate after merge\n",
        "print(\"Customer Order Summary:\")\n",
        "customer_summary = inner_merged.groupby(['CustomerID', 'Name', 'City']).agg({\n",
        "# .groupby(): Split data into groups based on column values for aggregation\n",
        "    'Amount': ['sum', 'mean', 'count'],\n",
        "    'OrderID': 'count'\n",
        "})\n",
        "print(customer_summary)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
